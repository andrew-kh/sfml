{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkillFactory\n",
    "## Введение в ML, введение в sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы с вами рассмотрим данные с конкурса [Задача предсказания отклика клиентов ОТП Банка](http://www.machinelearning.ru/wiki/index.php?title=%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D0%BA%D0%B0%D0%B7%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D1%82%D0%BA%D0%BB%D0%B8%D0%BA%D0%B0_%D0%BA%D0%BB%D0%B8%D0%B5%D0%BD%D1%82%D0%BE%D0%B2_%D0%9E%D0%A2%D0%9F_%D0%91%D0%B0%D0%BD%D0%BA%D0%B0_%28%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Грузим данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем описание данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_descr = pd.read_csv('data/otp_description.csv', sep='\\t', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(df_descr.shape[0]):\n",
    "#     print(df_descr.iloc[i,0], \"-\", df_descr.iloc[i,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем обучающую выборки и тестовую (которую мы как бы не видим)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/otp_train.csv', sep='\\t', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/otp_test.csv', sep='\\t', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Объединим две выборки\n",
    "\n",
    "Так как пока мы пока не умеем работать sklearn  Pipeline, то для того, чтобы после предобработки столбцы в двух выборках находились на своих местах.\n",
    "\n",
    "Для того, чтобы в дальнейшем отделить их введем новый столбец \"sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'sample'] = 'train'\n",
    "df_test.loc[:, 'sample'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_test.append(df_train).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df.pivot_table(\n",
    "    'AGREEMENT_RK', 'sample', 'TARGET', 'count')\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чуть-чуть посмотрим на данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Посмотрим типы данных и их заполняемость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Видим, что часть данных - object, скорее всего стоки.\n",
    "\n",
    "\n",
    "Давайте выведем эти значения для каждого столбца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_train.columns: # перебираем все столбцы\n",
    "    if str(df_train[i].dtype) == 'object': # если тип столбца - object\n",
    "        print('='*10)\n",
    "        print(i) # выводим название столбца\n",
    "        print(set(df_train[i])) # выводим все его значения (но делаем set - чтоб значения не повторялись)\n",
    "        print('\\n') # выводим пустую строку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Mожно заметить что некоторые переменные, которые обозначены как строки (например PERSONAL_INCOME) на самом деле числа, но по какой-то причине были распознаны как строки\n",
    "\n",
    "Причина же что использовалась запятая для разделения не целой части числа.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перекодировать их можно например так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df['PERSONAL_INCOME'].map(lambda x: x.replace(',', '.')).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой эффект наблюдается в столбцах `PERSONAL_INCOME`, `CREDIT`, `FST_PAYMENT`, `LOAN_AVG_DLQ_AMT`, `LOAN_MAX_DLQ_AMT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь ваше небольшое исследование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Задание 1. Есть ли пропуски в данных? Что с ними сделать?\n",
    "\n",
    "(единственного верного ответа нет - аргументируйте)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#посчитаем пропуски в данных:\n",
    "with_null = df.isnull().sum(axis = 0)\n",
    "with_null[with_null != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_решение:_ пропуски в данных есть, но способы их заполнения могут зависеть от причин.\n",
    "для этого рассмотрим пропуски по столбцам отдельно\n",
    "<br><br>\n",
    "На сайте сказано, что данные пустые, так как клиенты не заполнили эти поля, но в любом случае нужно проверить на зависимость от других параметром"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Факторы с одинаковым количеством пропусков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У факторов 'GEN_INDUSTRY', 'GEN_TITLE', 'ORG_TP_STATE', 'JOB_DIR' одинаковое количество пропусков. Проверим, находятся ли пропуски в одних и тех же строках. Например, есть ли в этих факторах не пустые значения, если значение 'GEN_INDUSTRY' пустое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['GEN_INDUSTRY'].isnull() == True][['GEN_INDUSTRY',\n",
    "                                         'GEN_TITLE',\n",
    "                                         'ORG_TP_STATE',\n",
    "                                         'JOB_DIR']].isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#создадим отдельный df и посмотрим его подробнее\n",
    "simult_na = df[df['GEN_INDUSTRY'].isnull() == True]\n",
    "simult_na.head(n = 15).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#скорее всего, данные не заполнены для всех неработающих и/или пенсионеров.\n",
    "#посчитаем к-во записей по столбцам SOCSTATUS_WORK_FL и SOCSTATUS_PENS_FL в\n",
    "#этом же df\n",
    "\n",
    "p = simult_na.pivot_table(\n",
    "    'TARGET', 'SOCSTATUS_WORK_FL', 'SOCSTATUS_PENS_FL', 'count')\n",
    "p\n",
    "#получается, что проблема была в отстутствии данных о работе у неработающих\n",
    "#пенсионеров. При этом безработные в выборке отсутствуют\n",
    "#также данных по перечисленным факторам нет для одного 24-летнего.\n",
    "#Предположим, что это ошибка в данных, тогда пропуск можно заполнить\n",
    "#самыми частыми значениями для его возрастной группы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Фактор worktime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже, пропуски в worktime тоже связаны с неработающими пенсионерами из предыдущего пункта. Однако пропусков там чуть больше, найдем в чем разница"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_worktime = df[df['WORK_TIME'].isnull() == True]\n",
    "o_worktime = pd.merge(no_worktime, simult_na, how='outer')\n",
    "i_worktime = pd.merge(no_worktime, simult_na)\n",
    "set_diff_worktime = pd.concat([o_worktime, i_worktime]).drop_duplicates(keep=False)\n",
    "#set_diff_worktime.T #в df 4 записи\n",
    "#есть 4 человека со статусом безработный пенсионер, у которых есть данные о работодателе\n",
    "#при этом не заполнены данные по времени работы в текущем месте. в остальном\n",
    "#этот фактор совпадает с 4 предыдущими"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Фактор ORG_TP_FCAPITAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#скорее всего, пропуски в этом факторе тоже связаны с отсутствием данных по безработным пенсионерам\n",
    "#поэтому найдем разницу между двумя df с пропусками в этих факторах\n",
    "\n",
    "no_capital = df[df['ORG_TP_FCAPITAL'].isnull() == True]\n",
    "o_capital = pd.merge(no_capital, simult_na, how='outer')\n",
    "i_capital = pd.merge(no_capital, simult_na)\n",
    "set_diff_capital = pd.concat([o_capital, i_capital]).drop_duplicates(keep=False)\n",
    "#set_diff_capital.T #7 записей в df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Два множества отличаются на 7 записей. Будем считать, что причина одинаковая"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Фактор TP_PROVINCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP_PROVINCE = область торговой точки, где клиент брал последний кредит\n",
    "из описания данных на сайте не совсем понятно, какие клиенты \"лежат\" в датасете. Значит ли это, что клиент не брал кредит или что данных нет по какой-то другой причине (например, кредит был получен не в магазине)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#проверим есть ли нули в поле CREDIT - это может означать, что в выборке есть клиенты без кредитов,\n",
    "#тогда для них не будет и торговой точки выдачи кредита.\n",
    "df[df['CREDIT'] == 0]\n",
    "\n",
    "#в поле с суммами выданных кредитов нет пропусков и нулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#в датафрейме кроме TP_PROVINCE есть еще 4 фактора, относящихся к географии,\n",
    "#и 4 фактора, указывающие на совпадение этих измерений (среди них есть и\n",
    "#совпадение по отсутствующему полю)\n",
    "#посмотрим этот набор более подробно:\n",
    "\n",
    "geo_col_list = ['REG_ADDRESS_PROVINCE', 'FACT_ADDRESS_PROVINCE', 'POSTAL_ADDRESS_PROVINCE',\n",
    "               'TP_PROVINCE', 'REGION_NM', 'REG_FACT_FL', 'REG_POST_FL', 'REG_FACT_POST_FL',\n",
    "               'REG_FACT_POST_TP_FL']\n",
    "\n",
    "geo_df = df[geo_col_list]\n",
    "geo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#отдельно посмотрим географические факторы с пропусками в TP_PROVINCE\n",
    "geo_df = geo_df[geo_df['TP_PROVINCE'].isnull() == True]\n",
    "geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#отдельно посмотрим есть ли в логической переменной REG_FACT_POST_TP_FL для TP_PROVINCE единицы\n",
    "\n",
    "geo_df[geo_df['REG_FACT_POST_TP_FL'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ответ на задание 1:* <br>\n",
    "пропуски есть. основные группы/факторы/причины:<br>\n",
    "1. __**проблема:**__ есть пропуски для неработающих пенсионеров в 6 столбцах (и совсем небольшое число пропусков там-же, которые можно списать на ошибки при сборе данных<br>\n",
    "__**решение:**__ можно сделать отдельный \"класс\" - неработающий пенсионер\n",
    "2. __**проблема:**__ 590 пропусков TP_PROVINCE - причина пока не ясна. <br>__**решение:**__ Но есть много других географических данных, можно заполнить данными из одного из них для каждого конкретного клиента. Делаем допущение, что часто они совпадают (нужно проверить!!)\n",
    "3. __**проблема:**__ PREVIOUS_CARD_NUM_UTILIZED <br>__**решение:**__ согласно описанию данных: пропуск = 0. просто заполним"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Задание 2. Есть ли категориальные признаки? Что с ними делать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#чтобы найти категориальные признаки - посмотрим к-во уникальных значений в каждом столбце (без учета nan)\n",
    "distinct_in_cols = {i : df[i].value_counts(dropna = True).count() for i in df.columns}\n",
    "distinct_in_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) в выдорке есть категориальные признаки, уже закодированные в приемлемом для обучения модели виде (1-0):\n",
    "already_binary = ['AUTO_RUS_FL', 'COT_PRESENCE_FL', 'FL_PRESENCE_FL', 'GAR_PRESENCE_FL', 'GENDER', 'GEN_PHONE_FL',\n",
    "                 'GPF_DOCUMENT_FL', 'HS_PRESENCE_FL', 'LAND_PRESENCE_FL',\n",
    "                 'REG_FACT_FL', 'REG_FACT_POST_FL', 'REG_FACT_POST_TP_FL', 'REG_PHONE_FL', 'REG_POST_FL',\n",
    "                 'SOCSTATUS_PENS_FL', 'SOCSTATUS_WORK_FL', 'FACT_PHONE_FL']\n",
    "\n",
    "#2) список категориальных признаков, которые будем кодировать one-hot:\n",
    "category_vars = ['EDUCATION', 'FAMILY_INCOME', 'ORG_TP_STATE']\n",
    "\n",
    "#непонятно LOAN_MAX_DLQ - номер просрочки качественный или нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Категориальные признаки с \"небольшим\" к-вом уникальных значений закодируем one-hot.\n",
    "2. Категориальные признаки, связанные с работой, закодируем численно (средний доход и возраст для отрасли, позиции и тд)\n",
    "3. Географию закодируем как средний доход по месту проживания, остальные \"текстовые\" поля удалим, оставим бинарные переменные, которые говорят о совпадении разных адресов клиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3. Фунция предобработки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию, которая бы\n",
    "\n",
    "* Удаляло идентификатор `AGREEMENT_RK`\n",
    "* Избавлялась от проблем с '.' и ',' в стобцах PERSONAL_INCOME, CREDIT, FST_PAYMENT, LOAN_AVG_DLQ_AMT, LOAN_MAX_DLQ_AMT\n",
    "* Что-то делала с пропусками\n",
    "* Кодировала категориальные признаки\n",
    "\n",
    "В результате, ваш датафрейм должен содержать только числа и не содержать пропусков!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#спасибо SO\n",
    "def code_mean(data, cat_feature, real_feature):\n",
    "    return (data[cat_feature].map(data.groupby(cat_feature)[real_feature].mean()))\n",
    "\n",
    "def code_median(data, cat_feature, real_feature):\n",
    "    return (data[cat_feature].map(data.groupby(cat_feature)[real_feature].median()))\n",
    "\n",
    "def preproc_data(df_input):\n",
    "    df_output = df_input.copy()\n",
    "    \n",
    "    #replace decimal sign\n",
    "    \n",
    "    lst_dec_repl = ['PERSONAL_INCOME', 'CREDIT', 'FST_PAYMENT', 'LOAN_AVG_DLQ_AMT', 'LOAN_MAX_DLQ_AMT']\n",
    "    \n",
    "    for i in lst_dec_repl:\n",
    "        df_output[i] = df_output[i].map(lambda x: x.replace(',', '.')).astype('float32')\n",
    "        \n",
    "    #нужно скорректировать отрицательные месяца и клиентов, которые\n",
    "    #веками (судя по данным) живут на одном месте. для них делаем \n",
    "    #допущение - всю жизь живет там же\n",
    "    df_output.loc[df_output['FACT_LIVING_TERM'] < 0, 'FACT_LIVING_TERM'] = 0\n",
    "    for index, row in df_output.iterrows():\n",
    "        if row['FACT_LIVING_TERM']/12 > row['AGE']:\n",
    "            row['FACT_LIVING_TERM'] > row['AGE']*12\n",
    "    \n",
    "    #есть люди, проработавшие на последнем месте больше своего возраста или\n",
    "    #люди, которые начали работать в 10 лет (судя по разнице текущего возраста\n",
    "    #и стажа на последнем месте). Предположим, самое раннее человек начал работать с 15:\n",
    "    for index, row in df_output.iterrows():\n",
    "        if np.isnan(row['WORK_TIME']) == False:\n",
    "            if (row['WORK_TIME']/12 > (row['AGE']-15)):\n",
    "                row['WORK_TIME'] = (row['AGE']-15)*12\n",
    "    \n",
    "    #заполняем пропуски\n",
    "    \n",
    "    #заполим пропуски для неработающих пенсионеров\n",
    "    subs_list = ['GEN_INDUSTRY', 'GEN_TITLE', 'ORG_TP_STATE', 'ORG_TP_FCAPITAL',\n",
    "            'JOB_DIR']\n",
    "\n",
    "    df_output.loc[(df_output['SOCSTATUS_PENS_FL'] == 1)\n",
    "           & (df_output['SOCSTATUS_WORK_FL'] == 0)\n",
    "           & (df_output['GEN_INDUSTRY'].isnull() == True), subs_list] = 'unemp_pens'\n",
    "    \n",
    "    #после этого останется еще один человек с пропусками в этих полях\n",
    "    #но в этом случае это скорее всего ошибка, поэтому заполним его\n",
    "    #пропуски самым частым значением для возраста до 31\n",
    "    dic_modes = {i : df_output.loc[:,i][df_output['AGE'] <= 30].value_counts().idxmax() for i in subs_list}\n",
    "    \n",
    "    for k, v in dic_modes.items():\n",
    "        df_output.loc[df_output['AGREEMENT_RK'] == 64469089, k] = dic_modes[k]\n",
    "    \n",
    "    #заполняем последний пропуск в ORG_TP_FCAPITAL\n",
    "    df_output.loc[df_output['ORG_TP_FCAPITAL'].isnull() == True, 'ORG_TP_FCAPITAL'] = df_output.loc[:,'ORG_TP_FCAPITAL'].value_counts().idxmax()\n",
    "    \n",
    "    #заполняем пропуски в REGION_NM и TP_PROVINCE из соседних столбцов (для остальных все совпадает)\n",
    "    df_output['REGION_NM'] = df_output['REGION_NM'].fillna(df_output['TP_PROVINCE'])\n",
    "    df_output['TP_PROVINCE'] = df_output['TP_PROVINCE'].fillna(df_output['FACT_ADDRESS_PROVINCE'])\n",
    "    \n",
    "    #заполняем WORK_TIME (уже после удаления \"выбросов\")\n",
    "    df_output['WORK_TIME'] = df_output['WORK_TIME'].fillna(df_output['WORK_TIME'].median())\n",
    "    \n",
    "    \n",
    "    #кодируем категориальные пременные\n",
    "    #укрупним и переименуем ряд признаков\n",
    "    dict_educ = {'Среднее специальное':'mid',\n",
    "             'Среднее':'mid',\n",
    "             'Высшее':'higher',\n",
    "             'Неполное среднее':'higher',\n",
    "             'Неоконченное высшее':'mid',\n",
    "             'Два и более высших образования':'higher',\n",
    "             'Ученая степень':'phd'}\n",
    "\n",
    "    dict_fam_inc = {'от 10000 до 20000 руб.':'10_20',\n",
    "                    'от 20000 до 50000 руб.':'20_50',\n",
    "                    'от 5000 до 10000 руб.':'5_10',\n",
    "                    'свыше 50000 руб.':'50_gr',\n",
    "                    'до 5000 руб.':'5_less'}\n",
    "    \n",
    "    dict_mar_stat = {'Состою в браке':'couple',\n",
    "                    'Гражданский брак':'couple',\n",
    "                    'Разведен(а)':'single',\n",
    "                    'Не состоял в браке':'single',\n",
    "                    'Вдовец/Вдова':'single'}\n",
    "    \n",
    "    dict_comp_type = {'Государственная комп./учреж.':'state',\n",
    "                     'Частная компания':'private_loc',\n",
    "                     'Индивидуальный предприниматель':'entrep',\n",
    "                     'Некоммерческая организация':'nfp',\n",
    "                     'Частная ком. с инос. капиталом':'private_glob'}\n",
    "\n",
    "    df_output['EDUCATION'].replace(dict_educ, inplace=True) #укрупняем и делаем one hot\n",
    "    df_output['FAMILY_INCOME'].replace(dict_fam_inc, inplace=True) #переводим и делаем one hot\n",
    "    df_output['MARITAL_STATUS'].replace(dict_mar_stat, inplace=True) #укрупняем чтобы сделать бинарным\n",
    "    df_output['MARITAL_STATUS'].replace({'single':0, 'couple':1}, inplace=True) \n",
    "    df_output['ORG_TP_STATE'].replace(dict_comp_type, inplace=True) #переводим\n",
    "    df_output['ORG_TP_FCAPITAL'].replace({'Без участия':0, 'С участием':1, 'unemp_pens':0}, inplace=True) #делаем бинарным\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df_output[['PERSONAL_INCOME']] = scaler.fit_transform(df_output[['PERSONAL_INCOME']])\n",
    "    df_output[['WORK_TIME']] = scaler.fit_transform(df_output[['WORK_TIME']])\n",
    "    \n",
    "    #географию \"закодируем\" медианным личным доходом в области фактического проживания клиента\n",
    "    #и 2 бинарными переменными: REG_FACT_POST_FL и REG_FACT_POST_TP_FL (остальные удалим)\n",
    "    df_output['FACT_PROV_INC'] = code_median(df_output, 'FACT_ADDRESS_PROVINCE', 'PERSONAL_INCOME')\n",
    "    df_output['GEN_INDUSTRY_INC'] = code_mean(df_output, 'GEN_INDUSTRY', 'PERSONAL_INCOME')\n",
    "    df_output['GEN_TITLE_AGE'] = code_mean(df_output, 'GEN_TITLE', 'AGE')\n",
    "    df_output['JOB_DIR_WORK'] = code_mean(df_output, 'JOB_DIR', 'WORK_TIME')\n",
    "    df_output['GEN_INDUSTRY_SIZE'] = df_output['GEN_INDUSTRY'].map(df_output.groupby('GEN_INDUSTRY').size())\n",
    "    df_output['GEN_TITLE_SIZE'] = df_output['GEN_TITLE'].map(df_output.groupby('GEN_TITLE').size())\n",
    "    df_output['JOB_DIR_SIZE'] = df_output['JOB_DIR'].map(df_output.groupby('JOB_DIR').size())\n",
    "            \n",
    "    \n",
    "#     lst_scale = ['FACT_PROV_INC', 'GEN_INDUSTRY_INC', 'CREDIT', 'GEN_INDUSTRY_SIZE',\n",
    "#                  'GEN_TITLE_SIZE','JOB_DIR_SIZE', 'GEN_TITLE_INC']\n",
    "    lst_scale = ['CREDIT','GEN_INDUSTRY_SIZE', 'GEN_TITLE_SIZE','JOB_DIR_SIZE',\n",
    "                 'FST_PAYMENT', 'FACT_LIVING_TERM', 'LOAN_MAX_DLQ_AMT', 'LOAN_MAX_DLQ_AMT']\n",
    "    df_output[lst_scale] = scaler.fit_transform(df_output[lst_scale])\n",
    "    \n",
    "    df_output = df_output.drop(['POSTAL_ADDRESS_PROVINCE',\n",
    "                                'REGION_NM',\n",
    "                                'REG_ADDRESS_PROVINCE',\n",
    "                                'TP_PROVINCE',\n",
    "                                'FACT_ADDRESS_PROVINCE',\n",
    "                                'GEN_INDUSTRY',\n",
    "                                'GEN_TITLE',\n",
    "                                'JOB_DIR'], axis=1)\n",
    "    \n",
    "    df_output = pd.get_dummies(df_output, columns=category_vars)\n",
    "    \n",
    "    df_output['PREVIOUS_CARD_NUM_UTILIZED'] = df_output['PREVIOUS_CARD_NUM_UTILIZED'].fillna(0)\n",
    "    \n",
    "    #удаляем лишние столбцы\n",
    "    df_output.drop('AGREEMENT_RK', axis=1, inplace=True)\n",
    "    df_output.drop('DL_DOCUMENT_FL', axis=1, inplace=True)\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_preproc = df.pipe(preproc_data)\n",
    "\n",
    "df_train_preproc = df_preproc.query('sample == \"train\"').drop(['sample'], axis=1)\n",
    "df_test_preproc = df_preproc.query('sample == \"test\"').drop(['sample'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4. Отделите целевую переменную и остальные признаки\n",
    "\n",
    "Должно получится:\n",
    "* 2 матрицы: X и X_test\n",
    "* 2 вектора: y и y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train_preproc['TARGET']\n",
    "y_test = df_test_preproc['TARGET']\n",
    "\n",
    "X = df_train_preproc.drop(['TARGET'], axis=1)\n",
    "X_test = df_test_preproc.drop(['TARGET'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preproc.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 5. Обучение и оценка качества разных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# test_size=0.3, random_state=42\n",
    "\n",
    "## Your Code Here\n",
    "X_Train, X_Test, y_Train, y_Test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Попробовать следующие \"черные ящики\": интерфейс одинаковый \n",
    "# Постепенно мы узнаем, что они делают а сейчас учимся понимать какой работает качественнее\n",
    "#     fit, \n",
    "#     predict, \n",
    "#     predict_proba\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_DTC = DecisionTreeClassifier()\n",
    "model_DTC.fit(X_Train, y_Train)\n",
    "predict_DTC = model_DTC.predict(X_test)\n",
    "predict_p_DTC = model_DTC.predict_proba(X_test)\n",
    "\n",
    "model_RFC = RandomForestClassifier()\n",
    "model_RFC.fit(X_Train, y_Train)\n",
    "predict_RFC = model_RFC.predict(X_test) \n",
    "predict_p_RFC = model_RFC.predict_proba(X_test) \n",
    "\n",
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(X_Train, y_Train)\n",
    "predict_LR = model_LR.predict(X_test)\n",
    "predict_p_LR = model_LR.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посчитать метрики стандартные\n",
    "# accuracy, precision, recall\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "## Your Code Here\n",
    "\n",
    "metric_names = ['Precision']*3 + ['Recall']*3 + ['Accuracy']*3\n",
    "\n",
    "d = {'model': ['DTC', 'RFC', 'LR']*3,\n",
    "     'metric' : metric_names,\n",
    "     'values': [precision_score(y_test, predict_DTC),\n",
    "                precision_score(y_test, predict_RFC),\n",
    "                precision_score(y_test, predict_LR),\n",
    "                recall_score(y_test, predict_DTC),\n",
    "                recall_score(y_test, predict_RFC),\n",
    "                recall_score(y_test, predict_LR),\n",
    "                accuracy_score(y_test, predict_DTC),\n",
    "                accuracy_score(y_test, predict_RFC),\n",
    "                accuracy_score(y_test, predict_LR)]\n",
    "    }\n",
    "\n",
    "metrics = pd.DataFrame(data=d)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуалищировать эти метрики всех моделей на одном графике (чтоб визуально сравнить)\n",
    "# Возможно вас удивит качество! Но задача подобрана специально ;) Такое качество тоже бывает\n",
    "\n",
    "## Your Code Here\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.barplot(x=\"metric\", y=\"values\", hue=\"model\", data=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "fpr_DTC, tpr_DTC, thresholds_DTC = roc_curve(y_test, predict_p_DTC[:,1])\n",
    "fpr_RFC, tpr_RFC, thresholds_RFC = roc_curve(y_test, predict_p_RFC[:,1])\n",
    "fpr_LR, tpr_LR, thresholds_LR = roc_curve(y_test, predict_p_LR[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Потроить roc-кривые всех можелей на одном графике\n",
    "# Вывести roc_auc каждой моделе\n",
    "# Подпишите оси и линии\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr_DTC, tpr_DTC)\n",
    "plt.plot(fpr_RFC, tpr_RFC)\n",
    "plt.plot(fpr_LR, tpr_LR)\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.ylabel('tpr')\n",
    "plt.xlabel('fpr')\n",
    "plt.grid(True)\n",
    "plt.title('ROC curve')\n",
    "plt.xlim((-0.01, 1.01))\n",
    "plt.ylim((-0.01, 1.01))\n",
    "plt.legend(['DTC', 'RFC', 'LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "# Сделать k-fold (10 фолдов) кросс-валидацию каждой модели\n",
    "# И посчитать средний roc_auc\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "print(np.mean(cross_val_score(\n",
    "    rfc,\n",
    "    X_Train,\n",
    "    y_Train,\n",
    "    scoring='roc_auc',\n",
    "    cv=10)))\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "print(np.mean(cross_val_score(\n",
    "    dtc,\n",
    "    X_Train,\n",
    "    y_Train,\n",
    "    scoring='roc_auc',\n",
    "    cv=10)))\n",
    "\n",
    "lr = LogisticRegression()\n",
    "print(np.mean(cross_val_score(\n",
    "    lr,\n",
    "    X_Train,\n",
    "    y_Train,\n",
    "    scoring='roc_auc',\n",
    "    cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Взять лучшую модель и сделать предсказания (с вероятностями (!!!)) для test выборки\n",
    "\n",
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(X_Train, y_Train)\n",
    "predict_LR = model_LR.predict(X_test)\n",
    "predict_p_LR = model_LR.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Померить roc_auc на тесте\n",
    "# Вывести текстом и на графике =)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr_LR, tpr_LR)\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.ylabel('tpr')\n",
    "plt.xlabel('fpr')\n",
    "plt.grid(True)\n",
    "plt.title('ROC curve')\n",
    "plt.xlim((-0.01, 1.01))\n",
    "plt.ylim((-0.01, 1.01))\n",
    "plt.legend(['LR'])\n",
    "roc_auc_score(y_test, predict_p_LR[:,1])\n",
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Больше обязательных заданий нет, но ниже вы можете провести исследование, поиграться с данными\n",
    "### Это возможность написать код, который я посмотрю и в случае чего откомметирую. ;)\n",
    "### Это не оценивается и остается на ваше усмотрение. Просто дополнительная возможность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**играл с данными и проиграл**</span>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "31px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
